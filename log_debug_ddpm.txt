nohup: ignoring input
2024-10-21 09:23:54.805507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-10-21 09:23:58.282200: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
/local2/fredxu/score_sde_gbm/utils.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_state = torch.load(ckpt_dir, map_location=device)
I1021 09:24:00.599944 139912069801088 xla_bridge.py:889] Unable to initialize backend 'cuda': 
I1021 09:24:00.600171 139912069801088 xla_bridge.py:889] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I1021 09:24:00.601043 139912069801088 xla_bridge.py:889] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
W1021 09:24:00.601230 139912069801088 xla_bridge.py:936] An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
I1021 09:24:00.981653 139912069801088 dataset_info.py:617] Load dataset info from /home/fredxu/tensorflow_datasets/cifar10/3.0.2
I1021 09:24:00.986782 139912069801088 dataset_info.py:709] For 'cifar10/3.0.2': fields info.[splits, supervised_keys, module_name] differ on disk and in the code. Keeping the one from code.
W1021 09:24:00.987088 139912069801088 options.py:599] options.experimental_threading is deprecated. Use options.threading instead.
W1021 09:24:00.987231 139912069801088 options.py:599] options.experimental_threading is deprecated. Use options.threading instead.
I1021 09:24:00.987488 139912069801088 dataset_builder.py:579] Reusing dataset cifar10 (/home/fredxu/tensorflow_datasets/cifar10/3.0.2)
I1021 09:24:00.988474 139912069801088 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /home/fredxu/tensorflow_datasets/cifar10/3.0.2.
I1021 09:24:01.101013 139912069801088 logging_logger.py:49] Constructing tf.data.Dataset cifar10 for split train, from /home/fredxu/tensorflow_datasets/cifar10/3.0.2
W1021 09:24:01.244541 139912069801088 options.py:599] options.experimental_threading is deprecated. Use options.threading instead.
W1021 09:24:01.244703 139912069801088 options.py:599] options.experimental_threading is deprecated. Use options.threading instead.
I1021 09:24:01.244920 139912069801088 dataset_builder.py:579] Reusing dataset cifar10 (/home/fredxu/tensorflow_datasets/cifar10/3.0.2)
I1021 09:24:01.245530 139912069801088 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /home/fredxu/tensorflow_datasets/cifar10/3.0.2.
I1021 09:24:01.285868 139912069801088 logging_logger.py:49] Constructing tf.data.Dataset cifar10 for split test, from /home/fredxu/tensorflow_datasets/cifar10/3.0.2
I1021 09:24:01.437721 139912069801088 run_lib.py:135] Starting training loop at step 50001.
I1021 09:24:24.796806 139912069801088 run_lib.py:150] step: 50050, training_loss: 1.95330e-02
I1021 09:24:47.519424 139912069801088 run_lib.py:150] step: 50100, training_loss: 2.33422e-02
I1021 09:24:48.021104 139912069801088 run_lib.py:167] step: 50100, eval_loss: 2.21583e-01
I1021 09:25:10.811106 139912069801088 run_lib.py:150] step: 50150, training_loss: 1.76732e-02
I1021 09:25:33.715862 139912069801088 run_lib.py:150] step: 50200, training_loss: 1.64271e-02
I1021 09:25:33.897341 139912069801088 run_lib.py:167] step: 50200, eval_loss: 1.98521e-01
I1021 09:25:56.831233 139912069801088 run_lib.py:150] step: 50250, training_loss: 1.89379e-02
I1021 09:26:19.793746 139912069801088 run_lib.py:150] step: 50300, training_loss: 1.71537e-02
I1021 09:26:19.973330 139912069801088 run_lib.py:167] step: 50300, eval_loss: 1.80547e-01
I1021 09:26:42.917494 139912069801088 run_lib.py:150] step: 50350, training_loss: 3.13077e-02
I1021 09:27:05.920910 139912069801088 run_lib.py:150] step: 50400, training_loss: 1.98903e-02
I1021 09:27:06.100055 139912069801088 run_lib.py:167] step: 50400, eval_loss: 1.62992e-01
I1021 09:27:29.128567 139912069801088 run_lib.py:150] step: 50450, training_loss: 2.23317e-02
I1021 09:27:52.159456 139912069801088 run_lib.py:150] step: 50500, training_loss: 2.29823e-02
I1021 09:27:52.338953 139912069801088 run_lib.py:167] step: 50500, eval_loss: 1.51858e-01
I1021 09:28:15.382930 139912069801088 run_lib.py:150] step: 50550, training_loss: 1.61013e-02
I1021 09:28:38.416597 139912069801088 run_lib.py:150] step: 50600, training_loss: 9.71241e-03
I1021 09:28:38.596037 139912069801088 run_lib.py:167] step: 50600, eval_loss: 1.42049e-01
I1021 09:29:01.652212 139912069801088 run_lib.py:150] step: 50650, training_loss: 2.08084e-02
I1021 09:29:24.717552 139912069801088 run_lib.py:150] step: 50700, training_loss: 1.28811e-02
I1021 09:29:24.897466 139912069801088 run_lib.py:167] step: 50700, eval_loss: 1.35079e-01
I1021 09:29:47.965840 139912069801088 run_lib.py:150] step: 50750, training_loss: 1.92596e-02
I1021 09:30:10.990928 139912069801088 run_lib.py:150] step: 50800, training_loss: 1.63537e-02
I1021 09:30:11.170373 139912069801088 run_lib.py:167] step: 50800, eval_loss: 1.25938e-01
I1021 09:30:34.251224 139912069801088 run_lib.py:150] step: 50850, training_loss: 1.65094e-02
I1021 09:30:57.332605 139912069801088 run_lib.py:150] step: 50900, training_loss: 1.79517e-02
I1021 09:30:57.512650 139912069801088 run_lib.py:167] step: 50900, eval_loss: 1.20268e-01
I1021 09:31:20.589221 139912069801088 run_lib.py:150] step: 50950, training_loss: 2.33372e-02
I1021 09:31:43.683213 139912069801088 run_lib.py:150] step: 51000, training_loss: 1.32484e-02
I1021 09:31:43.862421 139912069801088 run_lib.py:167] step: 51000, eval_loss: 1.14605e-01
I1021 09:32:06.935892 139912069801088 run_lib.py:150] step: 51050, training_loss: 2.04663e-02
I1021 09:32:29.989475 139912069801088 run_lib.py:150] step: 51100, training_loss: 1.66787e-02
I1021 09:32:30.168702 139912069801088 run_lib.py:167] step: 51100, eval_loss: 1.09406e-01
I1021 09:32:53.204295 139912069801088 run_lib.py:150] step: 51150, training_loss: 1.67276e-02
I1021 09:33:16.296078 139912069801088 run_lib.py:150] step: 51200, training_loss: 1.19275e-02
I1021 09:33:16.474824 139912069801088 run_lib.py:167] step: 51200, eval_loss: 1.05029e-01
I1021 09:33:39.565874 139912069801088 run_lib.py:150] step: 51250, training_loss: 1.96018e-02
I1021 09:34:02.646919 139912069801088 run_lib.py:150] step: 51300, training_loss: 1.77339e-02
I1021 09:34:02.826644 139912069801088 run_lib.py:167] step: 51300, eval_loss: 1.01414e-01
I1021 09:34:25.904076 139912069801088 run_lib.py:150] step: 51350, training_loss: 1.22618e-02
I1021 09:34:48.982332 139912069801088 run_lib.py:150] step: 51400, training_loss: 1.20152e-02
I1021 09:34:49.161056 139912069801088 run_lib.py:167] step: 51400, eval_loss: 9.88467e-02
I1021 09:35:12.224047 139912069801088 run_lib.py:150] step: 51450, training_loss: 1.71505e-02
I1021 09:35:35.281300 139912069801088 run_lib.py:150] step: 51500, training_loss: 1.28277e-02
I1021 09:35:35.460258 139912069801088 run_lib.py:167] step: 51500, eval_loss: 9.22929e-02
I1021 09:35:58.579264 139912069801088 run_lib.py:150] step: 51550, training_loss: 1.46813e-02
I1021 09:36:21.658555 139912069801088 run_lib.py:150] step: 51600, training_loss: 2.17524e-02
I1021 09:36:21.838219 139912069801088 run_lib.py:167] step: 51600, eval_loss: 9.29515e-02
I1021 09:36:44.962360 139912069801088 run_lib.py:150] step: 51650, training_loss: 1.26708e-02
I1021 09:37:08.031014 139912069801088 run_lib.py:150] step: 51700, training_loss: 2.37604e-02
I1021 09:37:08.208843 139912069801088 run_lib.py:167] step: 51700, eval_loss: 8.80505e-02
I1021 09:37:31.251340 139912069801088 run_lib.py:150] step: 51750, training_loss: 2.63758e-02
I1021 09:37:54.287410 139912069801088 run_lib.py:150] step: 51800, training_loss: 1.28332e-02
I1021 09:37:54.466046 139912069801088 run_lib.py:167] step: 51800, eval_loss: 8.52498e-02
I1021 09:38:17.515572 139912069801088 run_lib.py:150] step: 51850, training_loss: 1.68226e-02
I1021 09:38:40.564341 139912069801088 run_lib.py:150] step: 51900, training_loss: 2.08548e-02
I1021 09:38:40.743529 139912069801088 run_lib.py:167] step: 51900, eval_loss: 8.52528e-02
I1021 09:39:03.796175 139912069801088 run_lib.py:150] step: 51950, training_loss: 1.40344e-02
I1021 09:39:26.850020 139912069801088 run_lib.py:150] step: 52000, training_loss: 2.62697e-02
I1021 09:39:27.029844 139912069801088 run_lib.py:167] step: 52000, eval_loss: 8.28543e-02
I1021 09:39:50.078330 139912069801088 run_lib.py:150] step: 52050, training_loss: 2.33386e-02
I1021 09:40:13.141177 139912069801088 run_lib.py:150] step: 52100, training_loss: 1.77806e-02
I1021 09:40:13.319520 139912069801088 run_lib.py:167] step: 52100, eval_loss: 7.23930e-02
I1021 09:40:36.357773 139912069801088 run_lib.py:150] step: 52150, training_loss: 1.57320e-02
